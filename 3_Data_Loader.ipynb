{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84a51855",
   "metadata": {},
   "source": [
    "# ---------------------------------------构建用于训练的打包数据包-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235dc65a",
   "metadata": {},
   "source": [
    "- ### 处理好图片和词表后开始将数据打包用于训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3d9aece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import nltk\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "import argparse\n",
    "from torchvision import transforms\n",
    "\n",
    "# import Ipynb_importer\n",
    "import import_ipynb\n",
    "from Bulid_Vocab_2 import Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44804981",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"创建coco自定义数据类\"\"\"\n",
    "class CocoDataset(data.Dataset):\n",
    "    def __init__(self, root, json, vocab, transform=None):\n",
    "        # root: 图像文件的根目录\n",
    "        # json: COCO标注文件路径\n",
    "        # vocab: 词汇表包装器\n",
    "        # transform: 图像增广处理\n",
    "        self.root = root\n",
    "        self.coco = COCO(json)\n",
    "        self.ids = list(self.coco.anns.keys()) # 获取coco数据集中所有标注的唯一标识符anns并转化为列表\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index): # 类方法，用于根据索引获取值\n",
    "        coco = self.coco\n",
    "        vocab = self.vocab\n",
    "        ann_id = self.ids[index] # 根据索引获取相应的标注的唯一识别符\n",
    "        \n",
    "        caption = coco.anns[ann_id]['caption'] # 获取标注的文件，此处是针对图片的描述\n",
    "        img_id = coco.anns[ann_id]['image_id'] # 获取关联图像的唯一标识符\n",
    "        path = coco.loadImgs(img_id)[0]['file_name'] # 通过图像id获取对应图像的文件名\n",
    "        \n",
    "        image = Image.open(os.path.join(self.root, path)).convert('RGB')# 图像路径有root和path图片名组成，打开图像\n",
    "        \n",
    "        if self.transform is not None: # 判断是否需要对图片进行增广处理\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        tokens = nltk.tokenize.word_tokenize(str(caption).lower()) # 对标注文本进行分词，并转换为小写\n",
    "        caption = [] # 创建空字典\n",
    "        caption.append(vocab('<start>')) # 在标注开头添加start起始符对应的序号\n",
    "        caption.extend([vocab(token) for token in tokens]) # 遍历标注，将其转换为索引后加入到caption中\n",
    "        caption.append(vocab('<end>')) # 添加end结束符对应的序号\n",
    "        target = torch.Tensor(caption) # 将标注文件转化为Tensor\n",
    "        \n",
    "        return image, target  # 返回图片及其对应的处理好的标准文件\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ids) # 返回ids长度\n",
    "# 返回的是一个元组的列表，元组包括图片及其对应的处理好的标注文件，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25e78d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"创建用于训练的batch所需要的格式\"\"\"\n",
    "def collate_fn(data):\n",
    "    \"\"\"从图像和标注的元组列表中创建小批量张量。\n",
    "\n",
    "我们应该构建自定义的 `collate_fn`，而不是使用默认的 `collate_fn`，因为默认情况下不支持合并标注（包括填充）。\n",
    "\n",
    "参数:\n",
    "    data: 元组列表（图像, 标注）。\n",
    "        - 图像: 形状为 (3, 256, 256) 的 PyTorch 张量。\n",
    "        - 标注: 形状为 (?) 的 PyTorch 张量，可变长度。\n",
    "\n",
    "返回:\n",
    "    images: 形状为 (batch_size, 3, 256, 256) 的 PyTorch 张量。\n",
    "    targets: 形状为 (batch_size, padded_length) 的 PyTorch 张量。\n",
    "    lengths: 列表，每个填充标注的有效长度。\n",
    "\"\"\"\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True) # 按data中【1】标注的长度进行降序排列\n",
    "    images, captions = zip(*data) # 将排序后的data拆分成两个元组，images,captions\n",
    "\n",
    "    images = torch.stack(images, 0) # 将图像元组合并成一个张量，就是所有图片叠加在一起\n",
    "\n",
    "    lengths = [len(cap) for cap in captions] # 计算每个标注的长度\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long() # 创建一个全零张量，长度为标注的数量，宽度为最大标注的长度\n",
    "    for i, cap in enumerate(captions): # 此处是将每个标注填充到相同的长度\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end]\n",
    "    return images, targets, lengths\n",
    "\n",
    "# 将图片合成一个batch_size, 同时对应的标注文件整合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3add8166",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"构建加载数据集\"\"\"\n",
    "def get_loader(root, json, vocab, transform, batch_size, shuffle, num_workers):\n",
    "    # 这一步是实例化自定义数据集类，返回包含图片及其对应标注的元组的列表\n",
    "    coco = CocoDataset(root=root, json=json, vocab=vocab, transform=transform)\n",
    "    \n",
    "    # 将数据集打包\n",
    "    data_loader = torch.utils.data.DataLoader(dataset=coco,\n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=shuffle,\n",
    "                                             num_workers=num_workers,\n",
    "                                             collate_fn=collate_fn) # 自定义的collate_fn的方法\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b626e9d",
   "metadata": {},
   "source": [
    "# ----------------------------------------------测试能否正常运行------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a4aa98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(model_path='models/', crop_size=224, vocab_path='data/vocab.pkl', image_dir='data/resized2014', caption_path='data/annotations/captions_train2014.json', log_step=10, save_step=1000, embed_size=256, hidden_size=512, num_layers=1, num_epochs=1, batch_size=128, num_workers=None, learning_rate=0.001)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"设置参数\"\"\"\n",
    "parser = argparse.ArgumentParser() \n",
    "\n",
    "# 给定一些基础参数，包括文件路径等\n",
    "parser.add_argument('--model_path', type=str, default='models/', help='保存训练模型的地方')\n",
    "parser.add_argument('--crop_size', type=int, default=224, help='随机裁剪图片的大小')\n",
    "parser.add_argument('--vocab_path', type=str, default='data/vocab.pkl', help='之前生成词典的路径')\n",
    "parser.add_argument('--image_dir', type=str, default='data/resized2014', help='已经处理好大小的训练图片的路径')\n",
    "parser.add_argument('--caption_path', type=str, default='data/annotations/captions_train2014.json',help='训练集标签的路径')\n",
    "parser.add_argument('--log_step', type=int, default=10, help='打印训练进度的设定值')\n",
    "parser.add_argument('--save_step', type=int, default=1000, help='保存模型节点的设定值')\n",
    "\n",
    "# 设定模型的参数值\n",
    "parser.add_argument('--embed_size', type=int, default=256, help='词嵌入向量的维度，也就是用多少维来表示一个词元')\n",
    "parser.add_argument('--hidden_size', type=int, default=512, help='隐藏状态的维度')\n",
    "parser.add_argument('--num_layers', type=int, default=1, help='LSTM层的数量')\n",
    "\n",
    "# 设定训练的参数\n",
    "parser.add_argument('--num_epochs', type=int, default=1, help='epoch数')\n",
    "parser.add_argument('--batch_size', type=int, default=128, help='批量大小')\n",
    "parser.add_argument('--num_workers', type=int, default=None, help='并行运算大小')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.001, help='学习率')\n",
    "\n",
    "# 解析传入的命令行参数\n",
    "args = parser.parse_args(args=[])\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94b7eca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"定义增广操作\"\"\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomCrop(args.crop_size), # 随即裁剪到指定大小\n",
    "    transforms.RandomHorizontalFlip(), # 随机水平翻转\n",
    "    transforms.ToTensor(), # 转成tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))  # 标准化 \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de4f28ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"指定 vocab.pkl词表 文件的路径\"\"\"\n",
    "file_path = 'data/vocab.pkl'\n",
    "\n",
    "#使用 pickle 模块打开文件\n",
    "with open(file_path, 'rb') as file:\n",
    "     vocab = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dadda4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.49s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"测试能否正常生成批量数据\"\"\"\n",
    "data_loader = get_loader(args.image_dir, args.caption_path, vocab, transform, args.batch_size, \n",
    "                              shuffle=True, num_workers=0) #num_workers=args.num_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32c53c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets, lengths = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a20acad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 224, 224]) torch.Size([128, 25]) 128\n"
     ]
    }
   ],
   "source": [
    "print(images.shape, targets.shape, len(lengths))\n",
    "# images: 128批量，3通道，224*224图像大小\n",
    "# targets: 标注文件，128批量，统一填充成28个词元表示，每行中，每一个数字代表词元在字典中的索引\n",
    "# lengths: list，每个数字表示，当前行真实的标注是多少个，其它的都是填充pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd7b9f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 3, 224, 224])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e08b524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   1,  146,  364,  ..., 1997,   19,    2],\n",
      "        [   1,    4,  578,  ..., 2776,   19,    2],\n",
      "        [   1,    4, 2361,  ..., 4185,   19,    2],\n",
      "        ...,\n",
      "        [   1,  251,   53,  ...,    0,    0,    0],\n",
      "        [   1,  252,  108,  ...,    0,    0,    0],\n",
      "        [   1,    4,  116,  ...,    0,    0,    0]])\n"
     ]
    }
   ],
   "source": [
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd6f59a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 26])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0506d407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   1,    4,  860, 8422,   55,   33, 1929,    7,    4,   31, 1063, 1073,\n",
       "         968,    3,   87,  328,    3,    3, 7413, 8659,   87,    3,    3, 1075,\n",
       "          19,    2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[0,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e05536aa",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[26,\n",
       " 20,\n",
       " 18,\n",
       " 18,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 9]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75fecbfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8507c58b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c7ba80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_zzh",
   "language": "python",
   "name": "machine_zzh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
